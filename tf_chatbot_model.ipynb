{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# \"How to make a simple contextualized chatbot with tensorflow, keras, nltk and sklearn\"\n",
    "# \n",
    "#  by Andreas S. Rath <andreas.rath@ondewo.com> \n",
    "#  Github name: teddius\n",
    "#  Github source code: http://bit.ly/tfcb17ondewo\n",
    "#\n",
    "#  Inspired by chatbotsmagazine article which was based on \"tflearn\" and is available at \n",
    "#  https://chatbotsmagazine.com/contextual-chat-bots-with-tensorflow-4391749d0077\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Things we need for NLP\n",
    "######################################################################\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer() # english stemmer\n",
    "\n",
    "######################################################################\n",
    "# things we need for Tensorflow\n",
    "######################################################################\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.layers import Dense, Flatten, Conv1D, Embedding, MaxPooling1D, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Import our chat-bot intents file\n",
    "######################################################################\n",
    "import json\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Let's start to build our training data\n",
    "######################################################################\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        \n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "      \n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        \n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        \n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "print('-------------  Summary -------------------------------')\n",
    "print('------------------------------------------------------')\n",
    "print('')\n",
    "print(len(classes), \"classes\\n\", classes)\n",
    "print('')\n",
    "print('------------------------------------------------------')\n",
    "print('')\n",
    "print(len(words), \"words\\n\", words)\n",
    "print('')\n",
    "print('------------------------------------------------------')\n",
    "print('')\n",
    "print(len(documents), \"documents\\n\", documents)\n",
    "print('')\n",
    "print('------------------------------------------------------')\n",
    "print('')\n",
    "print(len(words), \"unique stemmed words\\n\", words)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "    \n",
    "# print('training: ' + str(training))\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "X = list(training[:, 0])\n",
    "y = list(training[:, 1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=32, shuffle=True)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "print('-------------  Summary -------------------------------')\n",
    "print('------------------------------------------------------')\n",
    "print('')\n",
    "print('Total elements in X: ' + str(len(X)) + ' consisting of') \n",
    "print('elements in X_train: ' + str(len(X_train)) + ' and X_test: ' + str(len(X_test)))\n",
    "print('')\n",
    "print('Total elements in y (labels): ' + str(len(y)) + ' consisting of') \n",
    "print('elements in y_train: ' + str(len(y_train)) + ' and y_test: ' + str(len(y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------------------------------------------')\n",
    "print('----- Let us look a specific training example --------')\n",
    "print('------------------------------------------------------')\n",
    "print('X_train[0] (bag of word references):', X_train[0])\n",
    "print('------------------------------------------------------')\n",
    "print('y_train[0] (class label):', y_train[0])\n",
    "print('------------------------------------------------------')\n",
    "print('All class labels:', classes)\n",
    "print('------------------------------------------------------')\n",
    "print('Our training example class label at index classes[' + str(y_train[0].index(1))+ ']=',\n",
    "      classes[y_train[0].index(1)])  # TODO show clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "####################################################################################\n",
    "# Build a very simple neural network\n",
    "####################################################################################\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\",input_dim=(np.array(X_train).shape[1])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# metrics\n",
    "adam = optimizers.Adam(lr=0.1, decay=0.005)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "####################################################################################\n",
    "# OPTIONAL for playing around you could add the following layers (watch out to \n",
    "# transform to correct shape)\n",
    "# \n",
    "# model.add(Embedding(len(words), embedding_vector_length))\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for the evaluation of the model\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs/')\n",
    "checkpoint = ModelCheckpoint('./weights-improvement-{epoch:02d}-{loss:.4f}.hdf5',\n",
    "                             monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#callbacks_list = [checkpoint, tensorboard_callback, early_stop]\n",
    "callbacks_list = [tensorboard_callback, early_stop]\n",
    "\n",
    "nr_of_epoches=100\n",
    "batch_size=32\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=nr_of_epoches,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Let's have a look at a single test example from X_test \n",
    "#############################################################\n",
    "print('')\n",
    "print('X_test[0] in total looks like:\\n\\n', X_test[0])\n",
    "print('')\n",
    "print('------------------------------------------------------')\n",
    "print('')\n",
    "print('X_test[0] has the class index stored at y_test[0] with the label: classes[' + str(y_test[0].index(1))+ ']=',\n",
    "      classes[y_test[0].index(1)]) \n",
    "print('')\n",
    "print('------------------------------------------------------')\n",
    "print('')\n",
    "prediction = model.predict(np.array([X_test[0]]))\n",
    "print('Total \"raw\" prediction for all classes looks like:\\n\\n', prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_THRESHOLD = 0.00000000001\n",
    "# generate probabilities from the model\n",
    "results = [[i, r] for i, r in enumerate(prediction[0]) if r > ERROR_THRESHOLD]\n",
    "print('Our prediction translated to classes and probabilities:\\n')\n",
    "for r in results:\n",
    "    print(classes[r[0]], round(r[1], 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# Let's define two needy functions to do the natural language preprocessing for us\n",
    "# and build the bag of words (bow) for us from a sentence of words\n",
    "###################################################################################\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('These are all the words for our classification:\\n\\n', words)\n",
    "print('')\n",
    "print('Our sentence is represented by the following bag of words (bow):\\n')\n",
    "p = bow(\"Can you please tell me if you are open today?\", words, show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Kindly reminder: our classes we want to predict are:\\n\\n', classes)\n",
    "\n",
    "ERROR_THRESHOLD=0.0001\n",
    "prediction = model.predict(np.array([p]))\n",
    "# generate probabilities from the model\n",
    "results = [[i, r] for i, r in enumerate(prediction[0]) if r > ERROR_THRESHOLD]\n",
    "\n",
    "print('\\nOur prediction is:\\n')\n",
    "for r in results:\n",
    "    print('=> ', classes[r[0]], round(r[1],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Let's create a needy data structure to \n",
    "# (1) hold and track the user context\n",
    "# (2) classifies our sentence to a class\n",
    "# (3) generates a contextualized response for a specific user \n",
    "#        based on 3 elements\n",
    "#       (a) class with highest prediction propability\n",
    "#       (b) a specific user id\n",
    "#       (c) context set\n",
    "########################################################################\n",
    "\n",
    "# (1) hold and track the user context\n",
    "context = {}\n",
    "\n",
    "# (2) classifies our sentence to a class\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict(np.array([bow(sentence, words)]))[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "# (3) generates a contextualized response for a specific user\n",
    "def response(sentence, user_id='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # set context for this intent if necessary\n",
    "                    if 'context_set' in i:\n",
    "                        if show_details: print ('context:', i['context_set'])\n",
    "                        context[user_id] = i['context_set']\n",
    "\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in i or \\\n",
    "                        (user_id in context and 'context_filter' in i and i['context_filter'] == context[user_id]):\n",
    "                        if show_details: print ('tag:', i['tag'])\n",
    "                        # a random response from the intent\n",
    "                        return print(random.choice(i['responses']))\n",
    "\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response('Hey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify('What are you opening times today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response('is your shop open today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So how does this context thing work?\n",
    "context = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response('Can we rent a moped?', user_id='Andreas', show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response('today', user_id='Andreas',show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response('Bye bye')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
